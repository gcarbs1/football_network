{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Network Creation from Football Event Data**\n",
    "\n",
    "## **Objective**\n",
    "Create complex networks from football event data to analyze team performance patterns.\n",
    "\n",
    "## **Steps**\n",
    "1. Load processed event data\n",
    "2. Filter ball-related events (passes, dribbles, shots, fouls won)\n",
    "3. Create passing networks for each team in each match\n",
    "4. Calculate comprehensive network metrics\n",
    "5. Export network data for analysis\n",
    "\n",
    "## **Output**\n",
    "- Network summary with metrics for each team/match\n",
    "- Network edges (pass connections between players)\n",
    "- Player positions (average field positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.exception import AmbiguousSolution\n",
    "from networkx.algorithms import community as nx_comm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed event data...\n",
      "✓ Data loaded: 7,209,091 records\n",
      "\n",
      "Available columns: ['match_id', 'period', 'index', 'timestamp', 'type', 'team', 'team_id', 'player', 'player_id', 'pass_outcome', 'pass_recipient', 'pass_recipient_id', 'location_x', 'location_y', 'home_or_away', 'home_abbrev_name', 'away_abbrev_name', 'home_goals', 'away_goals', 'score_momentum', 'game_state', 'scoresheet', 'score_final', 'final_result']\n"
     ]
    }
   ],
   "source": [
    "# Configure paths\n",
    "DATA_PATH = Path(\"../data\")\n",
    "PROCESSED_DATA_PATH = DATA_PATH / \"processed\"\n",
    "\n",
    "# Load processed event data\n",
    "print(\"Loading processed event data...\")\n",
    "events_df = pd.read_parquet(PROCESSED_DATA_PATH / \"events_processed.parquet\")\n",
    "print(f\"✓ Data loaded: {len(events_df):,} records\")\n",
    "print(f\"\\nAvailable columns: {events_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Event Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ On-ball events filtered: 2,193,999 records\n",
      "  Percentage of total: 30.4%\n",
      "\n",
      "Event type distribution:\n",
      "type\n",
      "Pass        2016542\n",
      "Dribble       69318\n",
      "Foul Won      57267\n",
      "Shot          50872\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define on-ball events\n",
    "ON_BALL_EVENTS = ['Pass', 'Dribble', 'Shot', 'Foul Won']\n",
    "\n",
    "# Filter on-ball events\n",
    "on_ball_events = events_df[events_df['type'].isin(ON_BALL_EVENTS)].copy()\n",
    "\n",
    "print(f\"✓ On-ball events filtered: {len(on_ball_events):,} records\")\n",
    "print(f\"  Percentage of total: {len(on_ball_events)/len(events_df)*100:.1f}%\")\n",
    "print(\"\\nEvent type distribution:\")\n",
    "print(on_ball_events['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Network Construction Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_player_positions(on_ball_events: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate average field positions for each player per match and team.\"\"\"\n",
    "    print(\"\\nCalculating player positions...\")\n",
    "    \n",
    "    player_positions = on_ball_events.groupby(\n",
    "        ['match_id', 'team', 'player_id', 'player']\n",
    "    ).agg({\n",
    "        'location_x': 'mean',\n",
    "        'location_y': 'mean',\n",
    "        'team_id': 'first',\n",
    "        'home_or_away': 'first',\n",
    "        'home_abbrev_name': 'first',\n",
    "        'away_abbrev_name': 'first',\n",
    "        'score_final': 'first',\n",
    "        'final_result': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"  ✓ Positions calculated for {len(player_positions)} player-match combinations\")\n",
    "    return player_positions\n",
    "\n",
    "\n",
    "def create_network_edges(on_ball_events: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create network edges from passing and individual action data.\"\"\"\n",
    "    print(\"\\nCreating network edges...\")\n",
    "    \n",
    "    network_edges = []\n",
    "    groups = on_ball_events.groupby(['match_id', 'team'])\n",
    "    total_groups = len(groups)\n",
    "    \n",
    "    for idx, ((match_id, team), group) in enumerate(groups):\n",
    "        # Count passes between players\n",
    "        pass_counts = defaultdict(int)\n",
    "        passes = group[group['type'] == 'Pass']\n",
    "        total_passes = len(passes)\n",
    "        \n",
    "        for _, row in passes.iterrows():\n",
    "            if pd.notna(row['player_id']) and pd.notna(row['pass_recipient_id']):\n",
    "                key = (row['player_id'], row['pass_recipient_id'])\n",
    "                pass_counts[key] += 1\n",
    "        \n",
    "        # Count individual actions (self-loops)\n",
    "        individual_counts = defaultdict(int)\n",
    "        individual_actions = group[group['type'].isin(['Shot', 'Carry', 'Dribble'])]\n",
    "        \n",
    "        for _, row in individual_actions.iterrows():\n",
    "            if pd.notna(row['player_id']):\n",
    "                individual_counts[row['player_id']] += 1\n",
    "        \n",
    "        # Create edge records for passes\n",
    "        for (source, target), count in pass_counts.items():\n",
    "            weight = count / total_passes if total_passes > 0 else 0\n",
    "            network_edges.append({\n",
    "                'match_id': match_id,\n",
    "                'team': team,\n",
    "                'source_id': source,\n",
    "                'target_id': target,\n",
    "                'weight': weight\n",
    "            })\n",
    "        \n",
    "        # Create edge records for individual actions (self-loops)\n",
    "        for player_id, count in individual_counts.items():\n",
    "            network_edges.append({\n",
    "                'match_id': match_id,\n",
    "                'team': team,\n",
    "                'source_id': player_id,\n",
    "                'target_id': player_id,\n",
    "                'weight': count\n",
    "            })\n",
    "        \n",
    "        # Progress update\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{total_groups} team-match combinations...\")\n",
    "    \n",
    "    edges_df = pd.DataFrame(network_edges)\n",
    "    print(f\"  ✓ Total edges created: {len(edges_df)}\")\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Network Metrics Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_network_metrics(on_ball_events: pd.DataFrame, \n",
    "                            edges_df: pd.DataFrame, \n",
    "                            player_positions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate comprehensive network metrics for each team-match combination.\"\"\"\n",
    "    print(\"\\nCalculating network metrics...\")\n",
    "    \n",
    "    network_summaries = []\n",
    "    groups = on_ball_events.groupby(['match_id', 'team'])\n",
    "    \n",
    "    for (match_id, team), group in groups:\n",
    "        if group.empty:\n",
    "            continue\n",
    "        \n",
    "        # Initialize metrics dictionary\n",
    "        first_row = group.iloc[0]\n",
    "        metrics = {\n",
    "            # Match information\n",
    "            'match_id': match_id,\n",
    "            'team': team,\n",
    "            'team_id': first_row.get('team_id'),\n",
    "            'home_or_away': first_row.get('home_or_away'),\n",
    "            'home_abbrev_name': first_row.get('home_abbrev_name'),\n",
    "            'away_abbrev_name': first_row.get('away_abbrev_name'),\n",
    "            'goal_in_match': check_goal_in_match(on_ball_events, match_id, team),\n",
    "            'score_final': get_match_final_score(on_ball_events, match_id),\n",
    "            'final_result': first_row.get('final_result'),\n",
    "            \n",
    "            # Initialize all metrics to zero\n",
    "            'edge_count': 0,\n",
    "            'network_density': 0,\n",
    "            'avg_in_degree': 0,\n",
    "            'std_in_degree': 0,\n",
    "            'avg_out_degree': 0,\n",
    "            'std_out_degree': 0,\n",
    "            'avg_betweenness': 0,\n",
    "            'std_betweenness': 0,\n",
    "            'max_betweenness': 0,\n",
    "            'avg_pagerank': 0,\n",
    "            'std_pagerank': 0,\n",
    "            'avg_eigenvector': 0,\n",
    "            'std_eigenvector': 0,\n",
    "            'avg_clustering': 0,\n",
    "            'std_clustering': 0,\n",
    "            'transitivity': 0,\n",
    "            'triangle_count': 0,\n",
    "            'reciprocity': 0,\n",
    "            'assortativity': 0,\n",
    "            'modularity': 0,\n",
    "            'num_cycles': 0,\n",
    "            'spectral_radius': 0,\n",
    "            'fiedler_value': 0,\n",
    "            'edge_weight_entropy': 0,\n",
    "            'avg_katz': 0,\n",
    "            'std_katz': 0,\n",
    "            'avg_harmonic_closeness': 0,\n",
    "            'std_harmonic_closeness': 0\n",
    "        }\n",
    "        \n",
    "        # Get edges for this team-match\n",
    "        team_edges = edges_df.query(\"match_id == @match_id and team == @team\")\n",
    "        # Exclude self-loops for main network\n",
    "        pass_edges = team_edges[team_edges.source_id != team_edges.target_id]\n",
    "        \n",
    "        if not pass_edges.empty:\n",
    "            # Create directed graph\n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Add nodes from player positions\n",
    "            team_players = player_positions.query(\"match_id == @match_id and team == @team\")\n",
    "            for _, player in team_players.iterrows():\n",
    "                G.add_node(player.player_id)\n",
    "            \n",
    "            # Add edges\n",
    "            for _, edge in pass_edges.iterrows():\n",
    "                G.add_edge(edge.source_id, edge.target_id, weight=edge.weight)\n",
    "            \n",
    "            # Calculate all metrics\n",
    "            metrics.update(calculate_basic_metrics(G))\n",
    "            metrics.update(calculate_centrality_metrics(G))\n",
    "            metrics.update(calculate_clustering_metrics(G))\n",
    "            metrics.update(calculate_spectral_metrics(G))\n",
    "            metrics.update(calculate_advanced_metrics(G))\n",
    "        \n",
    "        network_summaries.append(metrics)\n",
    "    \n",
    "    summary_df = pd.DataFrame(network_summaries)\n",
    "    print(f\"  ✓ Metrics calculated for {len(summary_df)} networks\")\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def check_goal_in_match(df: pd.DataFrame, match_id: int, team: str) -> bool:\n",
    "    \"\"\"Check if the team scored in the match.\"\"\"\n",
    "    team_data = df[(df['match_id'] == match_id) & (df['team'] == team)]\n",
    "    if team_data.empty:\n",
    "        return False\n",
    "    \n",
    "    side = team_data['home_or_away'].iloc[0]\n",
    "    if side == 'HOME':\n",
    "        goals = team_data['home_goals']\n",
    "    else:\n",
    "        goals = team_data['away_goals']\n",
    "    \n",
    "    return (goals.max() - goals.min()) > 0\n",
    "\n",
    "\n",
    "def get_match_final_score(df: pd.DataFrame, match_id: int) -> str:\n",
    "    \"\"\"Get the final score of the match.\"\"\"\n",
    "    match_data = df[df['match_id'] == match_id]\n",
    "    if match_data.empty:\n",
    "        return \"0 x 0\"\n",
    "    \n",
    "    # Use score_momentum from the last event\n",
    "    if 'index' in match_data.columns and 'score_momentum' in match_data.columns:\n",
    "        last_event = match_data.loc[match_data['index'] == match_data['index'].max()]\n",
    "        if not last_event.empty:\n",
    "            return last_event['score_momentum'].iloc[0]\n",
    "    \n",
    "    # Fallback to max goals\n",
    "    if 'home_goals' in match_data.columns and 'away_goals' in match_data.columns:\n",
    "        return f\"{match_data['home_goals'].max()} x {match_data['away_goals'].max()}\"\n",
    "    \n",
    "    return \"0 x 0\"\n",
    "\n",
    "\n",
    "def calculate_basic_metrics(G: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"Calculate basic network metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Edge count and density\n",
    "    metrics['edge_count'] = G.number_of_edges()\n",
    "    metrics['network_density'] = nx.density(G)\n",
    "    \n",
    "    # Degree metrics\n",
    "    in_degrees = np.array([d for _, d in G.in_degree(weight='weight')])\n",
    "    out_degrees = np.array([d for _, d in G.out_degree(weight='weight')])\n",
    "    \n",
    "    if len(in_degrees) > 0:\n",
    "        metrics['avg_in_degree'] = in_degrees.mean()\n",
    "        metrics['std_in_degree'] = in_degrees.std()\n",
    "        metrics['avg_out_degree'] = out_degrees.mean()\n",
    "        metrics['std_out_degree'] = out_degrees.std()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_centrality_metrics(G: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"Calculate centrality metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight', normalized=True)\n",
    "    betweenness_values = np.array(list(betweenness.values()))\n",
    "    \n",
    "    if len(betweenness_values) > 0:\n",
    "        metrics['avg_betweenness'] = betweenness_values.mean()\n",
    "        metrics['std_betweenness'] = betweenness_values.std()\n",
    "        metrics['max_betweenness'] = betweenness_values.max()\n",
    "    \n",
    "    # PageRank\n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    pagerank_values = np.array(list(pagerank.values()))\n",
    "    metrics['avg_pagerank'] = pagerank_values.mean()\n",
    "    metrics['std_pagerank'] = pagerank_values.std()\n",
    "    \n",
    "    # Eigenvector centrality (on undirected version)\n",
    "    G_undirected = G.to_undirected()\n",
    "    try:\n",
    "        eigenvector = nx.eigenvector_centrality_numpy(G_undirected, weight='weight')\n",
    "    except AmbiguousSolution:\n",
    "        # Handle disconnected components\n",
    "        components = list(nx.connected_components(G_undirected))\n",
    "        largest_component = max(components, key=len)\n",
    "        subgraph = G_undirected.subgraph(largest_component)\n",
    "        eigenvector_sub = nx.eigenvector_centrality_numpy(subgraph, weight='weight')\n",
    "        eigenvector = {n: eigenvector_sub.get(n, 0) for n in G_undirected.nodes()}\n",
    "    \n",
    "    eigenvector_values = np.array(list(eigenvector.values()))\n",
    "    metrics['avg_eigenvector'] = eigenvector_values.mean()\n",
    "    metrics['std_eigenvector'] = eigenvector_values.std()\n",
    "    \n",
    "    # Katz centrality\n",
    "    try:\n",
    "        katz = nx.katz_centrality_numpy(G, weight='weight', alpha=0.005)\n",
    "        katz_values = np.array(list(katz.values()))\n",
    "        metrics['avg_katz'] = katz_values.mean()\n",
    "        metrics['std_katz'] = katz_values.std()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Harmonic closeness centrality\n",
    "    harmonic = nx.harmonic_centrality(G)\n",
    "    harmonic_values = np.array(list(harmonic.values()))\n",
    "    metrics['avg_harmonic_closeness'] = harmonic_values.mean()\n",
    "    metrics['std_harmonic_closeness'] = harmonic_values.std()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_clustering_metrics(G: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"Calculate clustering and community metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    clustering = nx.clustering(G_undirected, weight='weight')\n",
    "    clustering_values = np.array(list(clustering.values()))\n",
    "    \n",
    "    if len(clustering_values) > 0:\n",
    "        metrics['avg_clustering'] = clustering_values.mean()\n",
    "        metrics['std_clustering'] = clustering_values.std()\n",
    "    \n",
    "    # Transitivity\n",
    "    metrics['transitivity'] = nx.transitivity(G_undirected)\n",
    "    \n",
    "    # Triangle count\n",
    "    triangles = nx.triangles(G_undirected)\n",
    "    metrics['triangle_count'] = sum(triangles.values()) // 3\n",
    "    \n",
    "    # Reciprocity (for directed graph)\n",
    "    metrics['reciprocity'] = nx.reciprocity(G) or 0\n",
    "    \n",
    "    # Assortativity\n",
    "    metrics['assortativity'] = nx.degree_assortativity_coefficient(G, weight='weight') or 0\n",
    "    \n",
    "    # Modularity\n",
    "    communities = list(nx_comm.greedy_modularity_communities(G_undirected, weight='weight'))\n",
    "    if communities:\n",
    "        metrics['modularity'] = nx_comm.modularity(G_undirected, communities, weight='weight')\n",
    "    \n",
    "    # Number of cycles\n",
    "    metrics['num_cycles'] = len(nx.cycle_basis(G_undirected))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_spectral_metrics(G: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"Calculate spectral graph metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Spectral radius\n",
    "    adjacency_matrix = nx.to_numpy_array(G_undirected, weight='weight')\n",
    "    eigenvalues = np.linalg.eigvals(adjacency_matrix)\n",
    "    \n",
    "    if len(eigenvalues) > 0:\n",
    "        metrics['spectral_radius'] = float(np.max(np.abs(eigenvalues)))\n",
    "    \n",
    "    # Fiedler value (algebraic connectivity)\n",
    "    laplacian_matrix = nx.normalized_laplacian_matrix(G_undirected, weight='weight').toarray()\n",
    "    laplacian_eigenvalues = np.linalg.eigvals(laplacian_matrix)\n",
    "    sorted_eigenvalues = np.sort(laplacian_eigenvalues)\n",
    "    \n",
    "    if len(sorted_eigenvalues) > 1:\n",
    "        metrics['fiedler_value'] = float(sorted_eigenvalues[1])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_advanced_metrics(G: nx.DiGraph) -> Dict[str, float]:\n",
    "    \"\"\"Calculate advanced network metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Edge weight entropy\n",
    "    weights = np.array([data['weight'] for _, _, data in G.edges(data=True)])\n",
    "    total_weight = weights.sum()\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        probabilities = weights / total_weight\n",
    "        non_zero_probs = probabilities[probabilities > 0]\n",
    "        metrics['edge_weight_entropy'] = float(-np.sum(non_zero_probs * np.log2(non_zero_probs)))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Network Creation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING FOOTBALL NETWORKS ===\n",
      "\n",
      "Calculating player positions...\n",
      "  ✓ Positions calculated for 57085 player-match combinations\n",
      "\n",
      "Creating network edges...\n",
      "  Processed 100/4050 team-match combinations...\n",
      "  Processed 200/4050 team-match combinations...\n",
      "  Processed 300/4050 team-match combinations...\n",
      "  Processed 400/4050 team-match combinations...\n",
      "  Processed 500/4050 team-match combinations...\n",
      "  Processed 600/4050 team-match combinations...\n",
      "  Processed 700/4050 team-match combinations...\n",
      "  Processed 800/4050 team-match combinations...\n",
      "  Processed 900/4050 team-match combinations...\n",
      "  Processed 1000/4050 team-match combinations...\n",
      "  Processed 1100/4050 team-match combinations...\n",
      "  Processed 1200/4050 team-match combinations...\n",
      "  Processed 1300/4050 team-match combinations...\n",
      "  Processed 1400/4050 team-match combinations...\n",
      "  Processed 1500/4050 team-match combinations...\n",
      "  Processed 1600/4050 team-match combinations...\n",
      "  Processed 1700/4050 team-match combinations...\n",
      "  Processed 1800/4050 team-match combinations...\n",
      "  Processed 1900/4050 team-match combinations...\n",
      "  Processed 2000/4050 team-match combinations...\n",
      "  Processed 2100/4050 team-match combinations...\n",
      "  Processed 2200/4050 team-match combinations...\n",
      "  Processed 2300/4050 team-match combinations...\n",
      "  Processed 2400/4050 team-match combinations...\n",
      "  Processed 2500/4050 team-match combinations...\n",
      "  Processed 2600/4050 team-match combinations...\n",
      "  Processed 2700/4050 team-match combinations...\n",
      "  Processed 2800/4050 team-match combinations...\n",
      "  Processed 2900/4050 team-match combinations...\n",
      "  Processed 3000/4050 team-match combinations...\n",
      "  Processed 3100/4050 team-match combinations...\n",
      "  Processed 3200/4050 team-match combinations...\n",
      "  Processed 3300/4050 team-match combinations...\n",
      "  Processed 3400/4050 team-match combinations...\n",
      "  Processed 3500/4050 team-match combinations...\n",
      "  Processed 3600/4050 team-match combinations...\n",
      "  Processed 3700/4050 team-match combinations...\n",
      "  Processed 3800/4050 team-match combinations...\n",
      "  Processed 3900/4050 team-match combinations...\n",
      "  Processed 4000/4050 team-match combinations...\n",
      "  ✓ Total edges created: 517683\n",
      "\n",
      "Calculating network metrics...\n",
      "  ✓ Metrics calculated for 4050 networks\n",
      "\n",
      "✓ Network creation completed!\n",
      "  Networks created: 4050\n",
      "  Total edges: 517683\n",
      "  Total player positions: 57085\n"
     ]
    }
   ],
   "source": [
    "def create_football_networks(on_ball_events: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Main pipeline to create football networks from event data.\"\"\"\n",
    "    print(\"\\n=== CREATING FOOTBALL NETWORKS ===\")\n",
    "    \n",
    "    # Step 1: Calculate player positions\n",
    "    player_positions = calculate_player_positions(on_ball_events)\n",
    "    \n",
    "    # Step 2: Create network edges\n",
    "    network_edges = create_network_edges(on_ball_events)\n",
    "    \n",
    "    # Step 3: Calculate network metrics\n",
    "    network_summary = calculate_network_metrics(on_ball_events, network_edges, player_positions)\n",
    "    \n",
    "    return network_summary, network_edges, player_positions\n",
    "\n",
    "\n",
    "# Execute network creation\n",
    "network_summary, network_edges, player_positions = create_football_networks(on_ball_events)\n",
    "\n",
    "print(\"\\n✓ Network creation completed!\")\n",
    "print(f\"  Networks created: {len(network_summary)}\")\n",
    "print(f\"  Total edges: {len(network_edges)}\")\n",
    "print(f\"  Total player positions: {len(player_positions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Data Preparation for Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Final result converted to binary format (Win=1, Loss=0)\n"
     ]
    }
   ],
   "source": [
    "# Convert final_result to binary (W=1, L=0)\n",
    "network_summary['final_result'] = network_summary['final_result'].map({'W': 1, 'L': 0})\n",
    "print(\"✓ Final result converted to binary format (Win=1, Loss=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Network Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NETWORK ANALYSIS ===\n",
      "\n",
      "Summary statistics for key network metrics:\n",
      "\n",
      "edge_count:\n",
      "  Mean: 118.3035\n",
      "  Median: 119.0000\n",
      "  Std Dev: 13.2752\n",
      "  Min: 61.0000\n",
      "  Max: 166.0000\n",
      "\n",
      "network_density:\n",
      "  Mean: 0.6401\n",
      "  Median: 0.6429\n",
      "  Std Dev: 0.0753\n",
      "  Min: 0.3250\n",
      "  Max: 0.8939\n",
      "\n",
      "avg_betweenness:\n",
      "  Mean: 0.0765\n",
      "  Median: 0.0760\n",
      "  Std Dev: 0.0091\n",
      "  Min: 0.0468\n",
      "  Max: 0.1170\n",
      "\n",
      "avg_pagerank:\n",
      "  Mean: 0.0710\n",
      "  Median: 0.0714\n",
      "  Std Dev: 0.0037\n",
      "  Min: 0.0588\n",
      "  Max: 0.0909\n",
      "\n",
      "avg_clustering:\n",
      "  Mean: 0.1738\n",
      "  Median: 0.1715\n",
      "  Std Dev: 0.0425\n",
      "  Min: 0.0569\n",
      "  Max: 0.3443\n",
      "\n",
      "transitivity:\n",
      "  Mean: 0.8303\n",
      "  Median: 0.8355\n",
      "  Std Dev: 0.0471\n",
      "  Min: 0.5899\n",
      "  Max: 0.9677\n",
      "\n",
      "\n",
      "Metrics comparison by match result:\n",
      "\n",
      "edge_count:\n",
      "  Wins - Mean: 117.1867, Std: 13.8986\n",
      "  Losses - Mean: 119.4202, Std: 12.5254\n",
      "\n",
      "network_density:\n",
      "  Wins - Mean: 0.6377, Std: 0.0785\n",
      "  Losses - Mean: 0.6425, Std: 0.0720\n",
      "\n",
      "avg_betweenness:\n",
      "  Wins - Mean: 0.0788, Std: 0.0092\n",
      "  Losses - Mean: 0.0742, Std: 0.0084\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== NETWORK ANALYSIS ===\")\n",
    "\n",
    "# Key metrics to analyze\n",
    "key_metrics = [\n",
    "    'edge_count', 'network_density', 'avg_betweenness', \n",
    "    'avg_pagerank', 'avg_clustering', 'transitivity'\n",
    "]\n",
    "\n",
    "print(\"\\nSummary statistics for key network metrics:\")\n",
    "for metric in key_metrics:\n",
    "    if metric in network_summary.columns:\n",
    "        values = network_summary[metric]\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Mean: {values.mean():.4f}\")\n",
    "        print(f\"  Median: {values.median():.4f}\")\n",
    "        print(f\"  Std Dev: {values.std():.4f}\")\n",
    "        print(f\"  Min: {values.min():.4f}\")\n",
    "        print(f\"  Max: {values.max():.4f}\")\n",
    "\n",
    "# Analyze by result\n",
    "print(\"\\n\\nMetrics comparison by match result:\")\n",
    "for metric in key_metrics[:3]:  # Show first 3 metrics\n",
    "    if metric in network_summary.columns:\n",
    "        wins = network_summary[network_summary['final_result'] == 1][metric]\n",
    "        losses = network_summary[network_summary['final_result'] == 0][metric]\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Wins - Mean: {wins.mean():.4f}, Std: {wins.std():.4f}\")\n",
    "        print(f\"  Losses - Mean: {losses.mean():.4f}, Std: {losses.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Export Network Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPORTING NETWORK DATA ===\n",
      "✓ Network summary saved to: ..\\data\\processed\\on_ball_networks.parquet\n",
      "  File size: 0.84 MB\n",
      "\n",
      "✓ Network edges saved to: ..\\data\\processed\\network_edges.parquet\n",
      "  File size: 2.51 MB\n",
      "\n",
      "✓ Player positions saved to: ..\\data\\processed\\player_positions.parquet\n",
      "  File size: 1.20 MB\n",
      "\n",
      "✓ All network data exported successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EXPORTING NETWORK DATA ===\")\n",
    "\n",
    "# Export network summary\n",
    "networks_path = PROCESSED_DATA_PATH / \"on_ball_networks.parquet\"\n",
    "network_summary.to_parquet(networks_path, index=False)\n",
    "print(f\"✓ Network summary saved to: {networks_path}\")\n",
    "print(f\"  File size: {networks_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Export network edges\n",
    "edges_path = PROCESSED_DATA_PATH / \"network_edges.parquet\"\n",
    "network_edges.to_parquet(edges_path, index=False)\n",
    "print(f\"\\n✓ Network edges saved to: {edges_path}\")\n",
    "print(f\"  File size: {edges_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Export player positions\n",
    "positions_path = PROCESSED_DATA_PATH / \"player_positions.parquet\"\n",
    "player_positions.to_parquet(positions_path, index=False)\n",
    "print(f\"\\n✓ Player positions saved to: {positions_path}\")\n",
    "print(f\"  File size: {positions_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "print(\"\\n✓ All network data exported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
