{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection from StatsBomb**\n",
    "\n",
    "## **Objective**\n",
    "This notebook collects football match and event data from StatsBomb's public API for analysis.\n",
    "\n",
    "## **Steps**\n",
    "1. Setup environment and import libraries\n",
    "2. Configure data storage paths\n",
    "3. Define competitions and seasons to collect\n",
    "4. Collect match data\n",
    "5. Collect event data for matches\n",
    "6. Process and clean collected data\n",
    "7. Export processed data\n",
    "\n",
    "## **Output**\n",
    "- Raw match data in parquet format\n",
    "- Raw event data in parquet format\n",
    "- Processed event data with enriched features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsbombpy import sb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories created:\n",
      "  Matches: ..\\data\\raw\\matches\n",
      "  Events: ..\\data\\raw\\events\n",
      "  Processed: ..\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"../data\")\n",
    "RAW_DATA_PATH = DATA_PATH / \"raw\"\n",
    "PROCESSED_DATA_PATH = DATA_PATH / \"processed\"\n",
    "MATCHES_PATH = RAW_DATA_PATH / \"matches\"\n",
    "EVENTS_PATH = RAW_DATA_PATH / \"events\"\n",
    "\n",
    "for path in [MATCHES_PATH, EVENTS_PATH, PROCESSED_DATA_PATH]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directories created:\")\n",
    "print(f\"  Matches: {MATCHES_PATH}\")\n",
    "print(f\"  Events: {EVENTS_PATH}\")\n",
    "print(f\"  Processed: {PROCESSED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Competition and Season Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total competitions/seasons to collect: 40\n"
     ]
    }
   ],
   "source": [
    "COMPETITIONS_SEASONS = [\n",
    "    # Bundesliga\n",
    "    (9, 281),    # 2023/2024\n",
    "    (9, 27),     # 2015/2016\n",
    "    \n",
    "    # Champions League \n",
    "    (16, 4),     # 2018/2019\n",
    "    (16, 1),     # 2017/2018\n",
    "    (16, 2),     # 2016/2017\n",
    "    (16, 27),    # 2015/2016\n",
    "    (16, 26),    # 2014/2015\n",
    "    (16, 25),    # 2013/2014\n",
    "    (16, 24),    # 2012/2013\n",
    "    (16, 23),    # 2011/2012\n",
    "    (16, 22),    # 2010/2011\n",
    "    (16, 21),    # 2009/2010\n",
    "    (16, 41),    # 2008/2009\n",
    "    (16, 39),    # 2006/2007\n",
    "    (16, 37),    # 2004/2005\n",
    "    (16, 44),    # 2003/2004\n",
    "    \n",
    "    # Copa AmÃ©rica\n",
    "    (223, 282),  # 2024\n",
    "    \n",
    "    # FIFA World Cup\n",
    "    (43, 106),   # 2022\n",
    "    (43, 3),     # 2018\n",
    "    \n",
    "    # La Liga \n",
    "    (11, 90),    # 2020/2021\n",
    "    (11, 42),    # 2019/2020\n",
    "    (11, 4),     # 2018/2019\n",
    "    (11, 1),     # 2017/2018\n",
    "    (11, 2),     # 2016/2017\n",
    "    (11, 27),    # 2015/2016\n",
    "    (11, 26),    # 2014/2015\n",
    "    (11, 25),    # 2013/2014\n",
    "    (11, 24),    # 2012/2013\n",
    "    (11, 23),    # 2011/2012\n",
    "    (11, 22),    # 2010/2011\n",
    "\n",
    "    # Indian Super League\n",
    "    (1238, 108), # 2021/2022\n",
    "\n",
    "    # Ligue 1\n",
    "    (7, 235),    # 2022/2023\n",
    "    (7, 108),    # 2021/2022\n",
    "    (7, 27),     # 2015/2016\n",
    "\n",
    "    # MLS\n",
    "    (44, 107),   # 2023\n",
    "\n",
    "    # African Cup of Nations\n",
    "    (1267, 107), # 2023\n",
    "\n",
    "    # UEFA Euro\n",
    "    (55, 282),   # 2024\n",
    "    (55, 43),    # 2020\n",
    "\n",
    "    # Premier League\n",
    "    (2, 27),     # 2015/2016\n",
    "\n",
    "    # Serie A\n",
    "    (12, 27),    # 2015/2016\n",
    "]\n",
    "\n",
    "print(f\"Total competitions/seasons to collect: {len(COMPETITIONS_SEASONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename(competition_id: int, season_id: int, data_type: str = \"matches\") -> str:\n",
    "    return f\"{data_type}_competition_{competition_id}_season_{season_id}.parquet\"\n",
    "\n",
    "\n",
    "def check_existing_files(path: Path, prefix: str) -> List[Tuple[int, int]]:\n",
    "    existing_pairs = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.parquet') and filename.startswith(prefix):\n",
    "            parts = filename.replace('.parquet', '').split('_')\n",
    "            if len(parts) >= 5:\n",
    "                try:\n",
    "                    competition_id = int(parts[2])\n",
    "                    season_id = int(parts[4])\n",
    "                    existing_pairs.append((competition_id, season_id))\n",
    "                except (ValueError, IndexError):\n",
    "                    print(f\"Warning: Invalid filename format: {filename}\")\n",
    "    \n",
    "    return existing_pairs\n",
    "\n",
    "\n",
    "def fetch_with_retry(fetch_function, *args, max_retries: int = 3, **kwargs) -> Optional[pd.DataFrame]:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            data = fetch_function(*args, **kwargs)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (attempt + 1) * 2\n",
    "                print(f\"  Attempt {attempt + 1} failed: {e}\")\n",
    "                print(f\"  Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"  Failed after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Match Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_match_data() -> pd.DataFrame:\n",
    "    existing_matches = check_existing_files(MATCHES_PATH, \"matches_competition\")\n",
    "    print(f\"Found {len(existing_matches)} existing match files.\")\n",
    "    \n",
    "    all_matches_dfs = []\n",
    "    \n",
    "    for competition_id, season_id in COMPETITIONS_SEASONS:\n",
    "        if (competition_id, season_id) in existing_matches:\n",
    "            print(f\"\\nLoading existing data: Competition {competition_id}, Season {season_id}\")\n",
    "            parquet_path = MATCHES_PATH / generate_filename(competition_id, season_id)\n",
    "            matches_df = pd.read_parquet(parquet_path)\n",
    "            all_matches_dfs.append(matches_df)\n",
    "        else:\n",
    "            print(f\"\\nFetching new data: Competition {competition_id}, Season {season_id}\")\n",
    "            matches_df = fetch_with_retry(\n",
    "                sb.matches,\n",
    "                competition_id=competition_id,\n",
    "                season_id=season_id\n",
    "            )\n",
    "            \n",
    "            if matches_df is not None:\n",
    "                parquet_path = MATCHES_PATH / generate_filename(competition_id, season_id)\n",
    "                matches_df.to_parquet(parquet_path, index=False)\n",
    "                print(f\"  Saved to: {parquet_path.name}\")\n",
    "                all_matches_dfs.append(matches_df)\n",
    "            \n",
    "            time.sleep(1) \n",
    "    \n",
    "    if all_matches_dfs:\n",
    "        all_matches = pd.concat(all_matches_dfs, ignore_index=True)\n",
    "        print(f\"\\nTotal matches collected: {len(all_matches)}\")\n",
    "        \n",
    "        combined_path = RAW_DATA_PATH / \"all_matches_combined.parquet\"\n",
    "        all_matches.to_parquet(combined_path, index=False)\n",
    "        print(f\"Combined file saved to: {combined_path}\")\n",
    "        \n",
    "        return all_matches\n",
    "    else:\n",
    "        print(\"\\nNo match data collected\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 existing match files.\n",
      "\n",
      "Loading existing data: Competition 9, Season 281\n",
      "\n",
      "Loading existing data: Competition 9, Season 27\n",
      "\n",
      "Loading existing data: Competition 16, Season 4\n",
      "\n",
      "Loading existing data: Competition 16, Season 1\n",
      "\n",
      "Loading existing data: Competition 16, Season 2\n",
      "\n",
      "Loading existing data: Competition 16, Season 27\n",
      "\n",
      "Loading existing data: Competition 16, Season 26\n",
      "\n",
      "Loading existing data: Competition 16, Season 25\n",
      "\n",
      "Loading existing data: Competition 16, Season 24\n",
      "\n",
      "Loading existing data: Competition 16, Season 23\n",
      "\n",
      "Loading existing data: Competition 16, Season 22\n",
      "\n",
      "Loading existing data: Competition 16, Season 21\n",
      "\n",
      "Loading existing data: Competition 16, Season 41\n",
      "\n",
      "Loading existing data: Competition 16, Season 39\n",
      "\n",
      "Loading existing data: Competition 16, Season 37\n",
      "\n",
      "Loading existing data: Competition 16, Season 44\n",
      "\n",
      "Loading existing data: Competition 223, Season 282\n",
      "\n",
      "Loading existing data: Competition 43, Season 106\n",
      "\n",
      "Loading existing data: Competition 43, Season 3\n",
      "\n",
      "Loading existing data: Competition 11, Season 90\n",
      "\n",
      "Loading existing data: Competition 11, Season 42\n",
      "\n",
      "Loading existing data: Competition 11, Season 4\n",
      "\n",
      "Loading existing data: Competition 11, Season 1\n",
      "\n",
      "Loading existing data: Competition 11, Season 2\n",
      "\n",
      "Loading existing data: Competition 11, Season 27\n",
      "\n",
      "Loading existing data: Competition 11, Season 26\n",
      "\n",
      "Loading existing data: Competition 11, Season 25\n",
      "\n",
      "Loading existing data: Competition 11, Season 24\n",
      "\n",
      "Loading existing data: Competition 11, Season 23\n",
      "\n",
      "Loading existing data: Competition 11, Season 22\n",
      "\n",
      "Loading existing data: Competition 1238, Season 108\n",
      "\n",
      "Loading existing data: Competition 7, Season 235\n",
      "\n",
      "Loading existing data: Competition 7, Season 108\n",
      "\n",
      "Loading existing data: Competition 7, Season 27\n",
      "\n",
      "Loading existing data: Competition 44, Season 107\n",
      "\n",
      "Loading existing data: Competition 1267, Season 107\n",
      "\n",
      "Loading existing data: Competition 55, Season 282\n",
      "\n",
      "Loading existing data: Competition 55, Season 43\n",
      "\n",
      "Loading existing data: Competition 2, Season 27\n",
      "\n",
      "Loading existing data: Competition 12, Season 27\n",
      "\n",
      "Total matches collected: 2707\n",
      "Combined file saved to: ..\\data\\raw\\all_matches_combined.parquet\n"
     ]
    }
   ],
   "source": [
    "all_matches = collect_match_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Event Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event_coordinates(events_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    events_df[['location_x', 'location_y']] = events_df['location'].apply(\n",
    "        lambda x: pd.Series(x) if isinstance(x, (tuple, list)) else pd.Series([np.nan, np.nan])\n",
    "    )\n",
    "\n",
    "    if 'pass_end_location' in events_df.columns:\n",
    "        events_df[['pass_end_location_x', 'pass_end_location_y']] = events_df['pass_end_location'].apply(\n",
    "            lambda x: pd.Series(x) if isinstance(x, (tuple, list)) else pd.Series([np.nan, np.nan])\n",
    "        )\n",
    "\n",
    "    if 'carry_end_location' in events_df.columns:\n",
    "        events_df[['carry_end_location_x', 'carry_end_location_y']] = events_df['carry_end_location'].apply(\n",
    "            lambda x: pd.Series(x) if isinstance(x, (tuple, list)) else pd.Series([np.nan, np.nan])\n",
    "        )\n",
    "    \n",
    "    return events_df\n",
    "\n",
    "\n",
    "def collect_event_data(match_ids: List[int]) -> pd.DataFrame:\n",
    "    existing_events = []\n",
    "    for filename in os.listdir(EVENTS_PATH):\n",
    "        if filename.endswith('.parquet') and filename.startswith('events_match_'):\n",
    "            try:\n",
    "                match_id = int(filename.replace('events_match_', '').replace('.parquet', ''))\n",
    "                existing_events.append(match_id)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    print(f\"Found {len(existing_events)} existing event files.\")\n",
    "    \n",
    "    all_events_dfs = []\n",
    "    total_matches = len(match_ids)\n",
    "    \n",
    "    for idx, match_id in enumerate(match_ids):\n",
    "        if match_id in existing_events:\n",
    "            events_path = EVENTS_PATH / f\"events_match_{match_id}.parquet\"\n",
    "            events_df = pd.read_parquet(events_path)\n",
    "            all_events_dfs.append(events_df)\n",
    "        else:\n",
    "            print(f\"\\nFetching events for match {match_id}\")\n",
    "            events_df = fetch_with_retry(sb.events, match_id=match_id)\n",
    "            \n",
    "            if events_df is not None:\n",
    "                events_df = process_event_coordinates(events_df)\n",
    "                \n",
    "                events_path = EVENTS_PATH / f\"events_match_{match_id}.parquet\"\n",
    "                events_df.to_parquet(events_path, index=False)\n",
    "                all_events_dfs.append(events_df)\n",
    "            \n",
    "            time.sleep(1) \n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"\\nProgress: {idx + 1}/{total_matches} matches processed\")\n",
    "    \n",
    "    if all_events_dfs:\n",
    "        print(\"\\nCombining all events...\")\n",
    "        all_events = pd.concat(all_events_dfs, ignore_index=True)\n",
    "        print(f\"Total events collected: {len(all_events)}\")\n",
    "\n",
    "        combined_path = RAW_DATA_PATH / \"all_events_combined.parquet\"\n",
    "        all_events.to_parquet(combined_path, index=False)\n",
    "        print(f\"Combined file saved to: {combined_path}\")\n",
    "        \n",
    "        return all_events\n",
    "    else:\n",
    "        print(\"\\nNo event data collected\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total matches with winner: 2035\n",
      "Found 2035 existing event files.\n",
      "\n",
      "Progress: 50/2035 matches processed\n",
      "\n",
      "Progress: 100/2035 matches processed\n",
      "\n",
      "Progress: 150/2035 matches processed\n",
      "\n",
      "Progress: 200/2035 matches processed\n",
      "\n",
      "Progress: 250/2035 matches processed\n",
      "\n",
      "Progress: 300/2035 matches processed\n",
      "\n",
      "Progress: 350/2035 matches processed\n",
      "\n",
      "Progress: 400/2035 matches processed\n",
      "\n",
      "Progress: 450/2035 matches processed\n",
      "\n",
      "Progress: 500/2035 matches processed\n",
      "\n",
      "Progress: 550/2035 matches processed\n",
      "\n",
      "Progress: 600/2035 matches processed\n",
      "\n",
      "Progress: 650/2035 matches processed\n",
      "\n",
      "Progress: 700/2035 matches processed\n",
      "\n",
      "Progress: 750/2035 matches processed\n",
      "\n",
      "Progress: 800/2035 matches processed\n",
      "\n",
      "Progress: 850/2035 matches processed\n",
      "\n",
      "Progress: 900/2035 matches processed\n",
      "\n",
      "Progress: 950/2035 matches processed\n",
      "\n",
      "Progress: 1000/2035 matches processed\n",
      "\n",
      "Progress: 1050/2035 matches processed\n",
      "\n",
      "Progress: 1100/2035 matches processed\n",
      "\n",
      "Progress: 1150/2035 matches processed\n",
      "\n",
      "Progress: 1200/2035 matches processed\n",
      "\n",
      "Progress: 1250/2035 matches processed\n",
      "\n",
      "Progress: 1300/2035 matches processed\n",
      "\n",
      "Progress: 1350/2035 matches processed\n",
      "\n",
      "Progress: 1400/2035 matches processed\n",
      "\n",
      "Progress: 1450/2035 matches processed\n",
      "\n",
      "Progress: 1500/2035 matches processed\n",
      "\n",
      "Progress: 1550/2035 matches processed\n",
      "\n",
      "Progress: 1600/2035 matches processed\n",
      "\n",
      "Progress: 1650/2035 matches processed\n",
      "\n",
      "Progress: 1700/2035 matches processed\n",
      "\n",
      "Progress: 1750/2035 matches processed\n",
      "\n",
      "Progress: 1800/2035 matches processed\n",
      "\n",
      "Progress: 1850/2035 matches processed\n",
      "\n",
      "Progress: 1900/2035 matches processed\n",
      "\n",
      "Progress: 1950/2035 matches processed\n",
      "\n",
      "Progress: 2000/2035 matches processed\n",
      "\n",
      "Combining all events...\n",
      "Total events collected: 7254305\n",
      "Combined file saved to: ..\\data\\raw\\all_events_combined.parquet\n"
     ]
    }
   ],
   "source": [
    "if not all_matches.empty:\n",
    "    non_draw_matches = all_matches[all_matches['home_score'] != all_matches['away_score']]\n",
    "    match_ids = non_draw_matches['match_id'].tolist()\n",
    "    print(f\"\\nTotal matches with winner: {len(match_ids)}\")\n",
    "    \n",
    "    all_events = collect_event_data(match_ids)\n",
    "else:\n",
    "    print(\"No matches available for event collection\")\n",
    "    all_events = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_columns(events_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    relevant_columns = [\n",
    "        'match_id', 'period', 'index', 'minute', 'type', 'team',\n",
    "        'location_x', 'location_y', 'timestamp', 'team_id', 'player', 'player_id',\n",
    "        'pass_outcome', 'pass_recipient', 'pass_recipient_id', 'shot_outcome'\n",
    "    ]\n",
    "    \n",
    "    existing_columns = [col for col in relevant_columns if col in events_df.columns]\n",
    "    \n",
    "    return events_df[existing_columns].copy()\n",
    "\n",
    "\n",
    "def add_match_context(events_df: pd.DataFrame, matches_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    events_df = events_df[events_df[\"period\"].isin([1, 2])].copy()\n",
    "    \n",
    "    home_away_map = {}\n",
    "    for _, row in matches_df.iterrows():\n",
    "        match_id = row[\"match_id\"]\n",
    "        home_away_map[(match_id, row[\"home_team\"])] = \"HOME\"\n",
    "        home_away_map[(match_id, row[\"away_team\"])] = \"AWAY\"\n",
    "\n",
    "    events_df[\"home_or_away\"] = events_df.apply(\n",
    "        lambda row: home_away_map.get((row[\"match_id\"], row[\"team\"]), \"UNKNOWN\"),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return events_df\n",
    "\n",
    "\n",
    "def add_score_tracking(events_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    events_df = events_df.sort_values([\"match_id\", \"period\", \"index\"]).copy()\n",
    "    \n",
    "    def extract_team_names(group):\n",
    "        home_team = group.loc[group[\"home_or_away\"] == \"HOME\", \"team\"].iloc[0] if (group[\"home_or_away\"] == \"HOME\").any() else np.nan\n",
    "        away_team = group.loc[group[\"home_or_away\"] == \"AWAY\", \"team\"].iloc[0] if (group[\"home_or_away\"] == \"AWAY\").any() else np.nan\n",
    "        return pd.Series({\"home_team_name\": home_team, \"away_team_name\": away_team})\n",
    "    \n",
    "    team_names = events_df.groupby(\"match_id\").apply(extract_team_names).reset_index()\n",
    "    events_df = events_df.merge(team_names, on=\"match_id\", how=\"left\")\n",
    "    \n",
    "    events_df['is_home_goal'] = (\n",
    "        (events_df['team'] == events_df['home_team_name']) &\n",
    "        ((events_df['shot_outcome'] == \"Goal\") | (events_df['type'] == \"Own Goal For\"))\n",
    "    ).astype(int)\n",
    "    \n",
    "    events_df['is_away_goal'] = (\n",
    "        (events_df['team'] == events_df['away_team_name']) &\n",
    "        ((events_df['shot_outcome'] == \"Goal\") | (events_df['type'] == \"Own Goal For\"))\n",
    "    ).astype(int)\n",
    "    \n",
    "    events_df['home_goals'] = events_df.groupby(\"match_id\")['is_home_goal'].cumsum()\n",
    "    events_df['away_goals'] = events_df.groupby(\"match_id\")['is_away_goal'].cumsum()\n",
    "    \n",
    "    events_df['score_momentum'] = events_df['home_goals'].astype(str) + \" x \" + events_df['away_goals'].astype(str)\n",
    "    \n",
    "    def determine_game_state(row):\n",
    "        if row['team'] == row['home_team_name']:\n",
    "            if row['home_goals'] > row['away_goals']:\n",
    "                return \"W\"\n",
    "            elif row['home_goals'] < row['away_goals']:\n",
    "                return \"L\"\n",
    "            else:\n",
    "                return \"D\"\n",
    "        elif row['team'] == row['away_team_name']:\n",
    "            if row['away_goals'] > row['home_goals']:\n",
    "                return \"W\"\n",
    "            elif row['away_goals'] < row['home_goals']:\n",
    "                return \"L\"\n",
    "            else:\n",
    "                return \"D\"\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    events_df['game_state'] = events_df.apply(determine_game_state, axis=1)\n",
    "    \n",
    "    final_scores = events_df.groupby(\"match_id\").agg({\n",
    "        'home_goals': 'last',\n",
    "        'away_goals': 'last',\n",
    "        'home_team_name': 'first',\n",
    "        'away_team_name': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    final_scores['score_final'] = final_scores['home_goals'].astype(str) + \" x \" + final_scores['away_goals'].astype(str)\n",
    "    final_scores['scoresheet'] = (\n",
    "        final_scores['home_team_name'] + \" \" +\n",
    "        final_scores['home_goals'].astype(str) + \" x \" +\n",
    "        final_scores['away_goals'].astype(str) + \" \" +\n",
    "        final_scores['away_team_name']\n",
    "    )\n",
    "    \n",
    "    events_df = events_df.merge(\n",
    "        final_scores[['match_id', 'scoresheet', 'score_final']],\n",
    "        on=\"match_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Final result\n",
    "    events_df = events_df.merge(\n",
    "        final_scores[['match_id', 'home_goals', 'away_goals']],\n",
    "        on=\"match_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_final\")\n",
    "    )\n",
    "    \n",
    "    def determine_final_result(row):\n",
    "        if row['home_goals_final'] < row['away_goals_final']:\n",
    "            if row['team'] == row['home_team_name']:\n",
    "                return \"L\"\n",
    "            elif row['team'] == row['away_team_name']:\n",
    "                return \"W\"\n",
    "        elif row['home_goals_final'] > row['away_goals_final']:\n",
    "            if row['team'] == row['home_team_name']:\n",
    "                return \"W\"\n",
    "            elif row['team'] == row['away_team_name']:\n",
    "                return \"L\"\n",
    "        else:\n",
    "            return \"D\"\n",
    "    \n",
    "    events_df['final_result'] = events_df.apply(determine_final_result, axis=1)\n",
    "    \n",
    "    columns_to_drop = [\n",
    "        \"is_home_goal\", \"is_away_goal\",\n",
    "        \"home_goals_final\", \"away_goals_final\"\n",
    "    ]\n",
    "    events_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    events_df.rename(columns={\n",
    "        'home_team_name': 'home_abbrev_name',\n",
    "        'away_team_name': 'away_abbrev_name'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    return events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event data...\n",
      "Selected 16 relevant columns\n",
      "Added home/away context\n",
      "Added score tracking\n",
      "Removed 10 drawn matches\n",
      "\n",
      "Final dataset: 7209091 events from 2025 matches\n"
     ]
    }
   ],
   "source": [
    "if not all_events.empty:\n",
    "    print(\"Processing event data...\")\n",
    "    \n",
    "    processed_events = select_relevant_columns(all_events)\n",
    "    print(f\"Selected {len(processed_events.columns)} relevant columns\")\n",
    "\n",
    "    processed_events = add_match_context(processed_events, all_matches)\n",
    "    print(\"Added home/away context\")\n",
    "    \n",
    "    processed_events = add_score_tracking(processed_events)\n",
    "    print(\"Added score tracking\")\n",
    "    \n",
    "    draw_matches = processed_events[processed_events['final_result'] == 'D']['match_id'].unique()\n",
    "    processed_events = processed_events[~processed_events['match_id'].isin(draw_matches)].copy()\n",
    "    print(f\"Removed {len(draw_matches)} drawn matches\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(processed_events)} events from {processed_events['match_id'].nunique()} matches\")\n",
    "else:\n",
    "    print(\"No events to process\")\n",
    "    processed_events = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SUMMARY ===\n",
      "\n",
      "Total records: 7,209,091\n",
      "Total matches: 2025\n",
      "Total teams: 216\n",
      "Total players: 5926\n",
      "\n",
      "Top 10 event types:\n",
      "type\n",
      "Pass             2016542\n",
      "Ball Receipt*    1898758\n",
      "Carry            1558817\n",
      "Pressure          651480\n",
      "Ball Recovery     205322\n",
      "Duel              151274\n",
      "Clearance          88580\n",
      "Block              76091\n",
      "Dribble            69318\n",
      "Goal Keeper        61360\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Result distribution:\n",
      "final_result\n",
      "W    2025\n",
      "L    2025\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if not processed_events.empty:\n",
    "    print(\"=== DATA SUMMARY ===\")\n",
    "    print(f\"\\nTotal records: {len(processed_events):,}\")\n",
    "    print(f\"Total matches: {processed_events['match_id'].nunique()}\")\n",
    "    print(f\"Total teams: {processed_events['team'].nunique()}\")\n",
    "    print(f\"Total players: {processed_events['player_id'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nTop 10 event types:\")\n",
    "    print(processed_events['type'].value_counts().head(10))\n",
    "    \n",
    "    print(\"\\nResult distribution:\")\n",
    "    result_dist = processed_events.groupby(['match_id', 'team', 'final_result'])['final_result'].first().value_counts()\n",
    "    print(result_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Export Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to: ..\\data\\processed\\events_processed.parquet\n",
      "File size: 103.52 MB\n"
     ]
    }
   ],
   "source": [
    "if not processed_events.empty:\n",
    "    final_columns = [\n",
    "        'match_id', 'period', 'index', 'timestamp', 'type',\n",
    "        'team', 'team_id', 'player', 'player_id', 'pass_outcome', 'pass_recipient',\n",
    "        'pass_recipient_id', 'location_x', 'location_y', 'home_or_away', \n",
    "        'home_abbrev_name', 'away_abbrev_name', 'home_goals', 'away_goals', \n",
    "        'score_momentum', 'game_state', 'scoresheet', 'score_final', 'final_result'\n",
    "    ]\n",
    "    \n",
    "    existing_final_columns = [col for col in final_columns if col in processed_events.columns]\n",
    "    processed_events = processed_events[existing_final_columns]\n",
    "\n",
    "    output_path = PROCESSED_DATA_PATH / \"events_processed.parquet\"\n",
    "    processed_events.to_parquet(output_path, index=False)\n",
    "\n",
    "    file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nProcessed data saved to: {output_path}\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"\\nNo data to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
